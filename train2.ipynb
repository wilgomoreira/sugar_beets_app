{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84324211",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f345d781",
   "metadata": {},
   "source": [
    "### Number of Images and Labels and its shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12b532af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total valid pairs found: 283\n",
      "\n",
      "Example Image Path: /mnt/0D6BEAD6291820B7/Wilgo/Datasets/sugar_beets/rgb/rgb_00023.png\n",
      "Example Mask Path:  /mnt/0D6BEAD6291820B7/Wilgo/Datasets/sugar_beets/labels/bonirob_2016-05-23-10-37-10_0_frame100_GroundTruth_color.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re \n",
    "\n",
    "images_dir = '/mnt/0D6BEAD6291820B7/Wilgo/Datasets/sugar_beets/rgb'\n",
    "masks_dir = '/mnt/0D6BEAD6291820B7/Wilgo/Datasets/sugar_beets/labels'\n",
    "\n",
    "mask_file_pattern = re.compile(r'frame(\\d+)_GroundTruth_color\\.png')\n",
    "\n",
    "mask_id_to_path_map = {}\n",
    "for mask_filename in os.listdir(masks_dir):\n",
    "    match = mask_file_pattern.search(mask_filename)\n",
    "    if match:\n",
    "        frame_id = match.group(1)\n",
    "        mask_id_to_path_map[frame_id] = os.path.join(masks_dir, mask_filename)\n",
    "\n",
    "image_paths = []\n",
    "mask_paths = []\n",
    "\n",
    "for image_filename in os.listdir(images_dir):\n",
    "   \n",
    "    if image_filename.startswith('rgb_') and image_filename.endswith('.png'):\n",
    "        try:\n",
    "         \n",
    "            img_id_str = os.path.splitext(image_filename)[0].split('_')[1]\n",
    "            frame_id_to_match = str(int(img_id_str))\n",
    "\n",
    "            if frame_id_to_match in mask_id_to_path_map:\n",
    "                image_paths.append(os.path.join(images_dir, image_filename))\n",
    "                mask_paths.append(mask_id_to_path_map[frame_id_to_match])\n",
    "\n",
    "        except (IndexError, ValueError):\n",
    "            print(f\"⚠️ Skipping unexpected image file format: {image_filename}\")\n",
    "\n",
    "image_paths.sort()\n",
    "mask_paths.sort()\n",
    "\n",
    "print(f\"✅ Total valid pairs found: {len(image_paths)}\")\n",
    "\n",
    "if image_paths:\n",
    "    print(f\"\\nExample Image Path: {image_paths[0]}\")\n",
    "    print(f\"Example Mask Path:  {mask_paths[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336ae241",
   "metadata": {},
   "source": [
    "### Plot the images and color masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad30f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 8))\n",
    "shown = 0  \n",
    "\n",
    "idx = 0\n",
    "while shown < 5 and idx < len(image_paths):\n",
    "    img_path = image_paths[idx]\n",
    "    mask_path = mask_paths[idx]\n",
    "\n",
    "    if os.path.exists(img_path) and os.path.exists(mask_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        mask = cv2.imread(mask_path)\n",
    "\n",
    "        if img is not None and mask is not None:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            axs[0, shown].imshow(img)\n",
    "            axs[0, shown].set_title(f\"Image {idx + 1}\")\n",
    "            axs[0, shown].axis('off')\n",
    "\n",
    "            axs[1, shown].imshow(mask)\n",
    "            axs[1, shown].set_title(f\"Mask {idx + 1}\")\n",
    "            axs[1, shown].axis('off')\n",
    "\n",
    "            shown += 1\n",
    "\n",
    "    idx += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5630d230",
   "metadata": {},
   "source": [
    "### Tranform color mask in class mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87496ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "COLOR_MAP = {\n",
    "    (0, 0, 0): 0,           \n",
    "    (255, 0, 0): 1,          \n",
    "    (0, 50, 255): 2,         \n",
    "    (255, 150, 0): 3,       \n",
    "    (255, 200, 0): 3         \n",
    "}\n",
    "\n",
    "def convert_mask_to_class(mask_rgb):\n",
    "    mask_class = np.zeros(mask_rgb.shape[:2], dtype=np.uint8)\n",
    "    for color, class_idx in COLOR_MAP.items():\n",
    "        matches = np.all(mask_rgb == color, axis=-1)\n",
    "        mask_class[matches] = class_idx\n",
    "    return mask_class\n",
    "\n",
    "masks_class = []\n",
    "\n",
    "\n",
    "for path in tqdm(mask_paths, desc=\"Converting masks\"):\n",
    "    mask_rgb = cv2.imread(path)\n",
    "    if mask_rgb is None:\n",
    "        continue\n",
    "    mask_rgb = cv2.cvtColor(mask_rgb, cv2.COLOR_BGR2RGB)\n",
    "    mask_class = convert_mask_to_class(mask_rgb)\n",
    "    masks_class.append(mask_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fd658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mask = masks_class[3]\n",
    "total_pixels = mask.size\n",
    "classes, counts = np.unique(mask, return_counts=True)\n",
    "\n",
    "print(\"Percentage distribution of classes:\")\n",
    "for cls, count in zip(classes, counts):\n",
    "    percent = (count / total_pixels) * 100\n",
    "    print(f\"Class {cls}: {percent:.2f}%\")\n",
    "\n",
    "print(\"Shape of the original mask (RGB):\", mask_rgb.shape)\n",
    "print(\"Shape of the converted mask (indexed):\", mask_class.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af5fc47",
   "metadata": {},
   "source": [
    "## Dataset and Traning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df8d9a5",
   "metadata": {},
   "source": [
    "### Creating dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9db3192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_img = []\n",
    "all_mask = []\n",
    "\n",
    "\n",
    "for img_path, mask_path in tqdm(zip(image_paths, mask_paths), total=len(image_paths), desc=\"Extracting images and masks\"):\n",
    "\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    mask_rgb = cv2.imread(mask_path)\n",
    "    mask_rgb = cv2.cvtColor(mask_rgb, cv2.COLOR_BGR2RGB)\n",
    "    mask_class = convert_mask_to_class(mask_rgb)\n",
    "\n",
    "    all_img.append(image)\n",
    "    all_mask.append(mask_class)\n",
    "\n",
    "print(f\"\\nTotal of images and masks: {len(all_img)}\")\n",
    "print(f\"Shape each one images: {all_img[0].shape}, mask: {all_mask[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab1c989",
   "metadata": {},
   "source": [
    "### Dataset (Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d95b89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import torchvision.models.segmentation as segmentation\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc\n",
    "\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, images, masks):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].astype(np.float32) / 255.0\n",
    "        mask = self.masks[idx].astype(np.int64)\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        return torch.tensor(image, dtype=torch.float32), torch.tensor(mask, dtype=torch.long)\n",
    "\n",
    "dataset = PatchDataset(all_img, all_mask)\n",
    "\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "calibration_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - calibration_size\n",
    "train_dataset, calibration_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, calibration_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "calibration_size_A = len(calibration_dataset) // 2\n",
    "calibration_size_B = len(calibration_dataset) - calibration_size_A\n",
    "calibration_A, calibration_B = random_split(\n",
    "    calibration_dataset, [calibration_size_A, calibration_size_B],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "batch_size = 2\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "calibration_loader_A = DataLoader(calibration_A, batch_size=batch_size, shuffle=False)\n",
    "calibration_loader_B = DataLoader(calibration_B, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}\")\n",
    "print(f\"Calibration A (Validação): {len(calibration_A)}\")\n",
    "print(f\"Calibration B (Conformal): {len(calibration_B)}\")\n",
    "print(f\"Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d883bba2",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2c9acd",
   "metadata": {},
   "source": [
    "#### DeepLabV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8de075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = segmentation.deeplabv3_mobilenet_v3_large(weights='DEFAULT')\n",
    "model.classifier[4] = nn.Conv2d(256, 4, kernel_size=(1, 1))\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss e otimizador\n",
    "class_weights = torch.tensor([0.5, 1.0, 2.0, 0.5]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Mixed precision\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Salvar melhor modelo\n",
    "def save_best_model(model, path, iou_score, best_iou):\n",
    "    if iou_score > best_iou:\n",
    "        print(f\"New best model! IoU: {iou_score:.4f}\")\n",
    "        torch.save(model.state_dict(), path)\n",
    "        return iou_score\n",
    "    return best_iou\n",
    "\n",
    "# Treinamento usando mixed precision\n",
    "def train_model(model, train_loader, val_loader, epochs=10):\n",
    "    best_iou = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n=== Epoch {epoch+1}/{epochs} ===\")\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for images, masks in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)['out']\n",
    "                loss = criterion(outputs, masks)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Libera memória\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        print(f\"Training Loss: {train_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        # Validação\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        iou_scores = np.zeros(4)\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(images)['out']\n",
    "                    loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                for cls in range(4):\n",
    "                    intersection = ((preds == cls) & (masks == cls)).sum().item()\n",
    "                    union = ((preds == cls) | (masks == cls)).sum().item()\n",
    "                    iou = intersection / union if union != 0 else 0\n",
    "                    iou_scores[cls] += iou\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "        iou_scores /= len(val_loader)\n",
    "        mean_iou = np.mean(iou_scores)\n",
    "        print(f\"Validation Loss: {val_loss/len(val_loader):.4f}\")\n",
    "        for cls_idx, iou_score in enumerate(iou_scores):\n",
    "            print(f\"Class {cls_idx} IoU: {iou_score:.4f}\")\n",
    "        print(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "\n",
    "        best_iou = save_best_model(model, \"best_model.pth\", mean_iou, best_iou)\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            print(f\"GPU memory used: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "# Rodar treinamento\n",
    "train_model(model, train_loader, calibration_loader_A, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bece6c8",
   "metadata": {},
   "source": [
    "### Save the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8e46e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def save_logits_masks_images(model, loader, save_path):\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_masks = []\n",
    "    all_images = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(loader, desc=\"Saving logits, masks and images\"):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)['out']\n",
    "\n",
    "            all_logits.append(outputs.cpu())\n",
    "            all_masks.append(masks.cpu())\n",
    "            all_images.append(images.cpu())\n",
    "\n",
    "    all_logits = torch.cat(all_logits)\n",
    "    all_masks = torch.cat(all_masks)\n",
    "    all_images = torch.cat(all_images)\n",
    "\n",
    "    save_dict = {\n",
    "        \"logits\": all_logits,\n",
    "        \"masks\": all_masks,\n",
    "        \"images\": all_images\n",
    "    }\n",
    "\n",
    "    torch.save(save_dict, save_path)\n",
    "    print(f\"Logits, masks e images have saved at: {save_path}\")\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_model.pth\", weights_only=True))\n",
    "save_logits_masks_images(model, calibration_loader_B, \"calibration_logits.pth\")\n",
    "save_logits_masks_images(model, test_loader, \"test_logits.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
